{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d59a7-ac35-48c7-9689-1cb227720649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras.layers import Input, Bidirectional, Dense, Conv1D, Conv3D, LSTM, Flatten, Attention, MultiHeadAttention, GlobalAveragePooling1D, Concatenate, Add, Dropout, Softmax\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from utilz import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b64b56-957e-4d30-9e14-7ae2446c3e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78d78998-d3cb-4f21-9f9e-45ccf1fbb3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_clip = load_features('./data/visual_clip.pkl')\n",
    "acoustic = load_features('./data/acoustic_wav2vec.pkl')\n",
    "bert_embs = load_features('./data/textual_bert.pkl')\n",
    "label = load_features('./data/labels.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5184f46f-ae68-457b-952a-dc51de7c1db1",
   "metadata": {},
   "source": [
    "# From pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af3a756-8b76-4b53-b2f0-c95935a528bc",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20ce14f8-db3f-40f5-a3a3-568eaa918061",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_Self(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(Attention_Self, self).__init__(**kwargs)\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.S = tf.keras.layers.Dense(1)\n",
    "        self.units = units\n",
    "\n",
    "    def call(self, features):\n",
    "        features_ = tf.expand_dims(features, 1)\n",
    "        v = self.W1(features)\n",
    "        q =  self.W2(features_)\n",
    "        score = tf.nn.tanh(q + v)\n",
    "        attention_weights = tf.nn.softmax(self.S(score), axis=1)\n",
    "        ATTN = attention_weights * (v)\n",
    "        ATTN = tf.reduce_sum(ATTN, axis=1)\n",
    "        \n",
    "        return ATTN\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super(Attention_Self, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb69f497-2d71-43e0-ac18-0f2ee04240d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 12:12:47.170728: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-18 12:12:47.172087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 12:12:47.172237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 12:12:47.172313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 12:12:47.493878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 12:12:47.494018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 12:12:47.494099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-18 12:12:47.494171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14217 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "WARNING:absl:`causal` argument is deprecated. Please use `use_causal_mask` in call() method to specify causal masking.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 10, 512)]    0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 128, 512)]   0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 36, 768)]    0           []                               \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 64)           147904      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 64)           123200      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " model_2 (Functional)           [(None, 64),         295040      ['input_3[0][0]']                \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256)          0           ['model[0][0]',                  \n",
      "                                                                  'model_1[0][0]',                \n",
      "                                                                  'model_2[0][0]',                \n",
      "                                                                  'model_2[0][1]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           16448       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3)            195         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 582,787\n",
      "Trainable params: 16,643\n",
      "Non-trainable params: 566,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# './res/V_model_CNNLSTM_clip.tf'\n",
    "vis_ipt = Input((10, 512))\n",
    "v_model = tf.keras.models.load_model('./res/V_model_CNNLSTM_clip.tf/')\n",
    "v_model = Model(inputs=v_model.inputs, outputs=v_model.layers[-2].output)\n",
    "for l in v_model.layers:\n",
    "    l.trainable = False\n",
    "vis_h = v_model(vis_ipt)\n",
    "\n",
    "# './res/A_model_Att-BLSTM_wav2vec_v2.tf'\n",
    "aud_ipt = Input((128,512))\n",
    "a_model = tf.keras.models.load_model('./res/A_model_Att-BLSTM_wav2vec_v2.tf/')\n",
    "a_model = Model(inputs=a_model.inputs, outputs=a_model.layers[-2].output)\n",
    "for l in a_model.layers:\n",
    "    l.trainable = False\n",
    "aud_h = a_model(aud_ipt)\n",
    "\n",
    "# './res/T_model_AttCNN_bert.tf'\n",
    "tex_ipt = Input((36,768))\n",
    "t_model = tf.keras.models.load_model('./res/T_model_AttCNN_bert.tf/')\n",
    "t_model = Model(inputs=t_model.inputs, outputs=[t_model.layers[-4].output,t_model.layers[-3].output])\n",
    "for l in t_model.layers:\n",
    "    l.trainable = False\n",
    "tex_q, tex_qv_attention = t_model(tex_ipt)\n",
    "\n",
    "h = Concatenate()([vis_h, aud_h, tex_q, tex_qv_attention])\n",
    "h = Dense(64)(h)\n",
    "res = Dense(3, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs=[vis_ipt, aud_ipt, tex_ipt], outputs=res)\n",
    "model.compile(optimizer='Adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics='acc')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ca5a41-3765-4c35-a837-01e6c5eab6a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 12:12:53.571905: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/86 [=>............................] - ETA: 0s - loss: 1.0731 - acc: 0.5625  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 12:12:53.913731: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/86 [============================>.] - ETA: 0s - loss: 0.5750 - acc: 0.7701"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_concate_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_concate_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 11s 101ms/step - loss: 0.5698 - acc: 0.7727 - val_loss: 0.9059 - val_acc: 0.6798\n",
      "Epoch 2/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3698 - acc: 0.8567 - val_loss: 0.9600 - val_acc: 0.6579\n",
      "Epoch 3/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.3404 - acc: 0.8677 - val_loss: 0.9898 - val_acc: 0.6623\n",
      "Epoch 4/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.3115 - acc: 0.8721 - val_loss: 1.0300 - val_acc: 0.6579\n",
      "Epoch 5/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.2894 - acc: 0.8845 - val_loss: 1.0941 - val_acc: 0.6732\n",
      "Epoch 6/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.2873 - acc: 0.8889 - val_loss: 1.1560 - val_acc: 0.6579\n",
      "Epoch 7/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.2755 - acc: 0.8904 - val_loss: 1.1826 - val_acc: 0.6623\n",
      "Epoch 8/30\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.2705 - acc: 0.8918 - val_loss: 1.1922 - val_acc: 0.6513\n",
      "Epoch 9/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.2523 - acc: 0.9072 - val_loss: 1.2117 - val_acc: 0.6469\n",
      "Epoch 10/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.2455 - acc: 0.9006 - val_loss: 1.2348 - val_acc: 0.6513\n",
      "Epoch 11/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.2375 - acc: 0.9094 - val_loss: 1.2821 - val_acc: 0.6579\n",
      "Epoch 12/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.2398 - acc: 0.9057 - val_loss: 1.3316 - val_acc: 0.6623\n",
      "Epoch 13/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.2366 - acc: 0.9130 - val_loss: 1.3145 - val_acc: 0.6404\n",
      "Epoch 14/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.2260 - acc: 0.9145 - val_loss: 1.3463 - val_acc: 0.6469\n",
      "Epoch 15/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.2205 - acc: 0.9152 - val_loss: 1.3473 - val_acc: 0.6294\n",
      "Epoch 16/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.2305 - acc: 0.9072 - val_loss: 1.3867 - val_acc: 0.6360\n",
      "Epoch 17/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.2121 - acc: 0.9240 - val_loss: 1.4156 - val_acc: 0.6447\n",
      "Epoch 18/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.2225 - acc: 0.9189 - val_loss: 1.4426 - val_acc: 0.6338\n",
      "Epoch 19/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.2083 - acc: 0.9232 - val_loss: 1.5485 - val_acc: 0.6338\n",
      "Epoch 20/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.2232 - acc: 0.9137 - val_loss: 1.5489 - val_acc: 0.6382\n",
      "Epoch 21/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.2110 - acc: 0.9196 - val_loss: 1.6103 - val_acc: 0.6382\n",
      "Epoch 22/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.2064 - acc: 0.9167 - val_loss: 1.5372 - val_acc: 0.6053\n",
      "Epoch 23/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.2054 - acc: 0.9181 - val_loss: 1.5405 - val_acc: 0.6272\n",
      "Epoch 24/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.2003 - acc: 0.9232 - val_loss: 1.6510 - val_acc: 0.5658\n",
      "Epoch 25/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.1936 - acc: 0.9298 - val_loss: 1.5717 - val_acc: 0.6338\n",
      "Epoch 26/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.1915 - acc: 0.9298 - val_loss: 1.5792 - val_acc: 0.6250\n",
      "Epoch 27/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.1954 - acc: 0.9203 - val_loss: 1.5967 - val_acc: 0.6184\n",
      "Epoch 28/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.1810 - acc: 0.9306 - val_loss: 1.6260 - val_acc: 0.6206\n",
      "Epoch 29/30\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.1770 - acc: 0.9306 - val_loss: 1.6407 - val_acc: 0.5921\n",
      "Epoch 30/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.1836 - acc: 0.9306 - val_loss: 1.6837 - val_acc: 0.6425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8005a5480>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_list = [ModelCheckpoint(filepath='./res/multi_model_concate_pretrained.tf', monitor='val_loss', save_best_only=True, save_freq='epoch')]\n",
    "\n",
    "model.fit(x=[np.asarray(visual_clip['train']), np.asarray(acoustic['train']), np.asarray(bert_embs['train'])], y=np.asarray(label['train']), batch_size=16, epochs=30, \n",
    "            validation_data=[[np.asarray(visual_clip['valid']), np.asarray(acoustic['valid']), np.asarray(bert_embs['valid'])], np.asarray(label['valid'])],\n",
    "            callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f59120ab-f66e-4d18-b27a-fb98bee8e9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg     0.7109    0.8427    0.7712       248\n",
      "         Pos     0.6288    0.5929    0.6103       140\n",
      "         Neu     0.3548    0.1594    0.2200        69\n",
      "\n",
      "    accuracy                         0.6630       457\n",
      "   macro avg     0.5648    0.5317    0.5338       457\n",
      "weighted avg     0.6320    0.6630    0.6387       457\n",
      "\n",
      "[[209  29  10]\n",
      " [ 47  83  10]\n",
      " [ 38  20  11]]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./res/multi_model_concate_pretrained.tf/')\n",
    "pred = model.predict([np.asarray(visual_clip['test']), np.asarray(acoustic['test']), np.asarray(bert_embs['test'])])\n",
    "predicted_test_labels = pred.argmax(axis=1)\n",
    "numeric_test_labels = np.array(label['test'])\n",
    "            \n",
    "eval_res = classification_report(numeric_test_labels, predicted_test_labels, \n",
    "                                    target_names = ['Neg', 'Pos', 'Neu'], \n",
    "                                    digits=4, output_dict=False)\n",
    "\n",
    "print(eval_res)\n",
    "\n",
    "cm = confusion_matrix(y_true=numeric_test_labels.tolist(), y_pred=predicted_test_labels.tolist())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f4f2ab-3e90-498b-8878-67471dc8dce2",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ff6f76-55a2-4e69-bbfe-92522ad6def2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:absl:`causal` argument is deprecated. Please use `use_causal_mask` in call() method to specify causal masking.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 10, 512)]    0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 128, 512)]   0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 36, 768)]    0           []                               \n",
      "                                                                                                  \n",
      " model_4 (Functional)           (None, 64)           147904      ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " model_5 (Functional)           (None, 64)           123200      ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " model_6 (Functional)           [(None, 64),         295040      ['input_6[0][0]']                \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 256)          0           ['model_4[0][0]',                \n",
      "                                                                  'model_5[0][0]',                \n",
      "                                                                  'model_6[0][0]',                \n",
      "                                                                  'model_6[0][1]']                \n",
      "                                                                                                  \n",
      " attention__self (Attention_Sel  (None, 256)         131841      ['concatenate_1[0][0]']          \n",
      " f)                                                                                               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           16448       ['attention__self[0][0]']        \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 3)            195         ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 714,628\n",
      "Trainable params: 148,484\n",
      "Non-trainable params: 566,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# './res/V_model_CNNLSTM_clip.tf'\n",
    "vis_ipt = Input((10, 512))\n",
    "v_model = tf.keras.models.load_model('./res/V_model_CNNLSTM_clip.tf/')\n",
    "v_model = Model(inputs=v_model.inputs, outputs=v_model.layers[-2].output)\n",
    "for l in v_model.layers:\n",
    "    l.trainable = False\n",
    "vis_h = v_model(vis_ipt)\n",
    "\n",
    "# './res/A_model_Att-BLSTM_wav2vec_v2.tf'\n",
    "aud_ipt = Input((128,512))\n",
    "a_model = tf.keras.models.load_model('./res/A_model_Att-BLSTM_wav2vec_v2.tf/')\n",
    "a_model = Model(inputs=a_model.inputs, outputs=a_model.layers[-2].output)\n",
    "for l in a_model.layers:\n",
    "    l.trainable = False\n",
    "aud_h = a_model(aud_ipt)\n",
    "\n",
    "# './res/T_model_AttCNN_bert.tf'\n",
    "tex_ipt = Input((36,768))\n",
    "t_model = tf.keras.models.load_model('./res/T_model_AttCNN_bert.tf/')\n",
    "t_model = Model(inputs=t_model.inputs, outputs=[t_model.layers[-4].output,t_model.layers[-3].output])\n",
    "for l in t_model.layers:\n",
    "    l.trainable = False\n",
    "tex_q, tex_qv_attention = t_model(tex_ipt)\n",
    "\n",
    "h = Concatenate()([vis_h, aud_h, tex_q, tex_qv_attention])\n",
    "h = Attention_Self(256)(h)\n",
    "h = Dense(64)(h)\n",
    "res = Dense(3, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs=[vis_ipt, aud_ipt, tex_ipt], outputs=res)\n",
    "model.compile(optimizer='Adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics='acc')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2277907c-a192-47b5-aa5a-0670d84f7b76",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.9312 - acc: 0.5762"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_selfatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_selfatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 11s 108ms/step - loss: 0.9165 - acc: 0.5855 - val_loss: 1.0227 - val_acc: 0.6118\n",
      "Epoch 2/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.6050 - acc: 0.7558 - val_loss: 1.0721 - val_acc: 0.6118\n",
      "Epoch 3/30\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5224 - acc: 0.7939"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_selfatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_selfatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 8s 95ms/step - loss: 0.5224 - acc: 0.7939 - val_loss: 1.0097 - val_acc: 0.6272\n",
      "Epoch 4/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4218 - acc: 0.8363 - val_loss: 1.0282 - val_acc: 0.6206\n",
      "Epoch 5/30\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4228 - acc: 0.8341"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_selfatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_selfatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 8s 94ms/step - loss: 0.4228 - acc: 0.8341 - val_loss: 0.9791 - val_acc: 0.6469\n",
      "Epoch 6/30\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.3894 - acc: 0.8545 - val_loss: 1.0413 - val_acc: 0.6294\n",
      "Epoch 7/30\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.3917 - acc: 0.8527"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_selfatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_selfatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 9s 100ms/step - loss: 0.3943 - acc: 0.8516 - val_loss: 0.9496 - val_acc: 0.6206\n",
      "Epoch 8/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3487 - acc: 0.8626 - val_loss: 0.9935 - val_acc: 0.6272\n",
      "Epoch 9/30\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.3179 - acc: 0.8750 - val_loss: 1.0675 - val_acc: 0.6623\n",
      "Epoch 10/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3170 - acc: 0.8816 - val_loss: 1.1124 - val_acc: 0.6053\n",
      "Epoch 11/30\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.3059 - acc: 0.8787 - val_loss: 1.0812 - val_acc: 0.6206\n",
      "Epoch 12/30\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.3124 - acc: 0.8816 - val_loss: 1.1041 - val_acc: 0.6272\n",
      "Epoch 13/30\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.2843 - acc: 0.8925 - val_loss: 1.2006 - val_acc: 0.6272\n",
      "Epoch 14/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2833 - acc: 0.8918 - val_loss: 1.0753 - val_acc: 0.6404\n",
      "Epoch 15/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.2689 - acc: 0.9057 - val_loss: 1.2733 - val_acc: 0.6009\n",
      "Epoch 16/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2764 - acc: 0.8991 - val_loss: 1.1981 - val_acc: 0.6250\n",
      "Epoch 17/30\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.2434 - acc: 0.9145 - val_loss: 1.1906 - val_acc: 0.6360\n",
      "Epoch 18/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2404 - acc: 0.9145 - val_loss: 1.2185 - val_acc: 0.6228\n",
      "Epoch 19/30\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.2411 - acc: 0.9115 - val_loss: 1.2174 - val_acc: 0.6206\n",
      "Epoch 20/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2180 - acc: 0.9211 - val_loss: 1.2557 - val_acc: 0.6382\n",
      "Epoch 21/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2311 - acc: 0.9167 - val_loss: 1.1205 - val_acc: 0.6447\n",
      "Epoch 22/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.1901 - acc: 0.9298 - val_loss: 1.4441 - val_acc: 0.6338\n",
      "Epoch 23/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.1878 - acc: 0.9298 - val_loss: 1.4326 - val_acc: 0.6272\n",
      "Epoch 24/30\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.2072 - acc: 0.9269 - val_loss: 1.4599 - val_acc: 0.6009\n",
      "Epoch 25/30\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 0.1643 - acc: 0.9393 - val_loss: 1.6105 - val_acc: 0.6184\n",
      "Epoch 26/30\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.1770 - acc: 0.9291 - val_loss: 1.4035 - val_acc: 0.6294\n",
      "Epoch 27/30\n",
      "86/86 [==============================] - 1s 17ms/step - loss: 0.1465 - acc: 0.9452 - val_loss: 1.5562 - val_acc: 0.6513\n",
      "Epoch 28/30\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.1764 - acc: 0.9371 - val_loss: 1.5793 - val_acc: 0.6338\n",
      "Epoch 29/30\n",
      "86/86 [==============================] - 1s 17ms/step - loss: 0.1153 - acc: 0.9539 - val_loss: 1.7077 - val_acc: 0.6382\n",
      "Epoch 30/30\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.1500 - acc: 0.9510 - val_loss: 1.6904 - val_acc: 0.6447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe77e1d95a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_list = [ModelCheckpoint(filepath='./res/multi_model_selfatt_pretrained.tf', monitor='val_loss', save_best_only=True, save_freq='epoch')]\n",
    "\n",
    "model.fit(x=[np.asarray(visual_clip['train']), np.asarray(acoustic['train']), np.asarray(bert_embs['train'])], y=np.asarray(label['train']), batch_size=16, epochs=30, \n",
    "            validation_data=[[np.asarray(visual_clip['valid']), np.asarray(acoustic['valid']), np.asarray(bert_embs['valid'])], np.asarray(label['valid'])],\n",
    "            callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "109ddb73-db2e-4773-840f-bff6940f3ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg     0.7319    0.8145    0.7710       248\n",
      "         Pos     0.5759    0.6500    0.6107       140\n",
      "         Neu     0.3913    0.1304    0.1957        69\n",
      "\n",
      "    accuracy                         0.6608       457\n",
      "   macro avg     0.5664    0.5317    0.5258       457\n",
      "weighted avg     0.6327    0.6608    0.6350       457\n",
      "\n",
      "[[202  42   4]\n",
      " [ 39  91  10]\n",
      " [ 35  25   9]]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./res/multi_model_selfatt_pretrained.tf/')\n",
    "pred = model.predict([np.asarray(visual_clip['test']), np.asarray(acoustic['test']), np.asarray(bert_embs['test'])])\n",
    "predicted_test_labels = pred.argmax(axis=1)\n",
    "numeric_test_labels = np.array(label['test'])\n",
    "            \n",
    "eval_res = classification_report(numeric_test_labels, predicted_test_labels, \n",
    "                                    target_names = ['Neg', 'Pos', 'Neu'], \n",
    "                                    digits=4, output_dict=False)\n",
    "\n",
    "print(eval_res)\n",
    "\n",
    "cm = confusion_matrix(y_true=numeric_test_labels.tolist(), y_pred=predicted_test_labels.tolist())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "137981d9-6ff1-42c8-999c-875d2658362d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:absl:`causal` argument is deprecated. Please use `use_causal_mask` in call() method to specify causal masking.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 10, 512)]    0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 128, 512)]   0           []                               \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, 36, 768)]    0           []                               \n",
      "                                                                                                  \n",
      " model_8 (Functional)           (None, 64)           147904      ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " model_9 (Functional)           (None, 64)           123200      ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " model_10 (Functional)          [(None, 64),         295040      ['input_9[0][0]']                \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 256)          0           ['model_8[0][0]',                \n",
      "                                                                  'model_9[0][0]',                \n",
      "                                                                  'model_10[0][0]',               \n",
      "                                                                  'model_10[0][1]']               \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 256)          1           ['concatenate_2[0][0]',          \n",
      "                                                                  'concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 64)           16448       ['attention[0][0]']              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 3)            195         ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 582,788\n",
      "Trainable params: 16,644\n",
      "Non-trainable params: 566,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# './res/V_model_CNNLSTM_clip.tf'\n",
    "vis_ipt = Input((10, 512))\n",
    "v_model = tf.keras.models.load_model('./res/V_model_CNNLSTM_clip.tf/')\n",
    "v_model = Model(inputs=v_model.inputs, outputs=v_model.layers[-2].output)\n",
    "for l in v_model.layers:\n",
    "    l.trainable = False\n",
    "vis_h = v_model(vis_ipt)\n",
    "\n",
    "# './res/A_model_Att-BLSTM_wav2vec_v2.tf'\n",
    "aud_ipt = Input((128,512))\n",
    "a_model = tf.keras.models.load_model('./res/A_model_Att-BLSTM_wav2vec_v2.tf/')\n",
    "a_model = Model(inputs=a_model.inputs, outputs=a_model.layers[-2].output)\n",
    "for l in a_model.layers:\n",
    "    l.trainable = False\n",
    "aud_h = a_model(aud_ipt)\n",
    "\n",
    "# './res/T_model_AttCNN_bert.tf'\n",
    "tex_ipt = Input((36,768))\n",
    "t_model = tf.keras.models.load_model('./res/T_model_AttCNN_bert.tf/')\n",
    "t_model = Model(inputs=t_model.inputs, outputs=[t_model.layers[-4].output,t_model.layers[-3].output])\n",
    "for l in t_model.layers:\n",
    "    l.trainable = False\n",
    "tex_q, tex_qv_attention = t_model(tex_ipt)\n",
    "\n",
    "h = Concatenate()([vis_h, aud_h, tex_q, tex_qv_attention])\n",
    "h = Attention(256)([h,h])\n",
    "h = Dense(64)(h)\n",
    "res = Dense(3, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs=[vis_ipt, aud_ipt, tex_ipt], outputs=res)\n",
    "model.compile(optimizer='Adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics='acc')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b147b8c-8164-4e07-8945-7ea853698743",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.7108 - acc: 0.7195"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_selfattv2_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_selfattv2_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 10s 103ms/step - loss: 0.7062 - acc: 0.7208 - val_loss: 0.9437 - val_acc: 0.6162\n",
      "Epoch 2/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5702 - acc: 0.8041 - val_loss: 1.0280 - val_acc: 0.6491\n",
      "Epoch 3/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.5810 - acc: 0.7931 - val_loss: 0.9602 - val_acc: 0.6228\n",
      "Epoch 4/30\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5404 - acc: 0.8048 - val_loss: 1.0127 - val_acc: 0.6250\n",
      "Epoch 5/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5488 - acc: 0.8099 - val_loss: 1.0040 - val_acc: 0.6316\n",
      "Epoch 6/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5706 - acc: 0.7946 - val_loss: 1.0112 - val_acc: 0.5987\n",
      "Epoch 7/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5761 - acc: 0.7931 - val_loss: 1.0332 - val_acc: 0.5921\n",
      "Epoch 8/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5484 - acc: 0.8041 - val_loss: 1.0323 - val_acc: 0.5724\n",
      "Epoch 9/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5528 - acc: 0.8012 - val_loss: 1.0200 - val_acc: 0.5943\n",
      "Epoch 10/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5236 - acc: 0.8026 - val_loss: 1.0316 - val_acc: 0.5768\n",
      "Epoch 11/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5393 - acc: 0.8034 - val_loss: 1.0490 - val_acc: 0.5614\n",
      "Epoch 12/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.5271 - acc: 0.8114 - val_loss: 1.0396 - val_acc: 0.5921\n",
      "Epoch 13/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.5280 - acc: 0.8194 - val_loss: 1.0328 - val_acc: 0.6075\n",
      "Epoch 14/30\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.5163 - acc: 0.8158 - val_loss: 1.0429 - val_acc: 0.5965\n",
      "Epoch 15/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.4840 - acc: 0.8143 - val_loss: 1.0731 - val_acc: 0.5987\n",
      "Epoch 16/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.5004 - acc: 0.8114 - val_loss: 1.0785 - val_acc: 0.5504\n",
      "Epoch 17/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5058 - acc: 0.8085 - val_loss: 1.0709 - val_acc: 0.6096\n",
      "Epoch 18/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.5084 - acc: 0.8107 - val_loss: 1.0659 - val_acc: 0.6053\n",
      "Epoch 19/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.5168 - acc: 0.8114 - val_loss: 1.0534 - val_acc: 0.6184\n",
      "Epoch 20/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5082 - acc: 0.8165 - val_loss: 1.0755 - val_acc: 0.6075\n",
      "Epoch 21/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.4954 - acc: 0.8114 - val_loss: 1.1072 - val_acc: 0.5724\n",
      "Epoch 22/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.5369 - acc: 0.8121 - val_loss: 1.1163 - val_acc: 0.5285\n",
      "Epoch 23/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5079 - acc: 0.8034 - val_loss: 1.0595 - val_acc: 0.5899\n",
      "Epoch 24/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5073 - acc: 0.8297 - val_loss: 1.0856 - val_acc: 0.5811\n",
      "Epoch 25/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4935 - acc: 0.8238 - val_loss: 1.1363 - val_acc: 0.5329\n",
      "Epoch 26/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4737 - acc: 0.8311 - val_loss: 1.0948 - val_acc: 0.6009\n",
      "Epoch 27/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.4965 - acc: 0.8129 - val_loss: 1.0787 - val_acc: 0.6140\n",
      "Epoch 28/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4776 - acc: 0.8202 - val_loss: 1.1101 - val_acc: 0.6031\n",
      "Epoch 29/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.4811 - acc: 0.8187 - val_loss: 1.1103 - val_acc: 0.5548\n",
      "Epoch 30/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.5147 - acc: 0.8092 - val_loss: 1.1170 - val_acc: 0.6184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe359ef6770>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_list = [ModelCheckpoint(filepath='./res/multi_model_selfattv2_pretrained.tf', monitor='val_loss', save_best_only=True, save_freq='epoch')]\n",
    "\n",
    "model.fit(x=[np.asarray(visual_clip['train']), np.asarray(acoustic['train']), np.asarray(bert_embs['train'])], y=np.asarray(label['train']), batch_size=16, epochs=30, \n",
    "            validation_data=[[np.asarray(visual_clip['valid']), np.asarray(acoustic['valid']), np.asarray(bert_embs['valid'])], np.asarray(label['valid'])],\n",
    "            callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b75cff87-6b18-47ab-845e-959b058649d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg     0.6906    0.7742    0.7300       248\n",
      "         Pos     0.5306    0.5571    0.5436       140\n",
      "         Neu     0.3750    0.1739    0.2376        69\n",
      "\n",
      "    accuracy                         0.6171       457\n",
      "   macro avg     0.5321    0.5017    0.5037       457\n",
      "weighted avg     0.5940    0.6171    0.5986       457\n",
      "\n",
      "[[192  48   8]\n",
      " [ 50  78  12]\n",
      " [ 36  21  12]]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./res/multi_model_selfattv2_pretrained.tf/')\n",
    "pred = model.predict([np.asarray(visual_clip['test']), np.asarray(acoustic['test']), np.asarray(bert_embs['test'])])\n",
    "predicted_test_labels = pred.argmax(axis=1)\n",
    "numeric_test_labels = np.array(label['test'])\n",
    "            \n",
    "eval_res = classification_report(numeric_test_labels, predicted_test_labels, \n",
    "                                    target_names = ['Neg', 'Pos', 'Neu'], \n",
    "                                    digits=4, output_dict=False)\n",
    "\n",
    "print(eval_res)\n",
    "\n",
    "cm = confusion_matrix(y_true=numeric_test_labels.tolist(), y_pred=predicted_test_labels.tolist())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c3e9a1-fec8-4dfd-a389-8a8b0993ef6c",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "330bf42e-fa89-436f-8525-a448f5551012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_Cross(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(Attention_Cross, self).__init__(**kwargs)\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.S = tf.keras.layers.Dense(1)\n",
    "        self.units = units\n",
    "\n",
    "    def call(self, features1, features2):\n",
    "        features2_ = tf.expand_dims(features2, 1)\n",
    "        v = self.W1(features1)\n",
    "        # v_ = self.W1(features2)\n",
    "        q =  self.W2(features2_)\n",
    "        score = tf.nn.tanh(q + v)\n",
    "        attention_weights = tf.nn.softmax(self.S(score), axis=1)\n",
    "        # ATTN = attention_weights * (v+v_)\n",
    "        ATTN = attention_weights * v\n",
    "        ATTN = tf.reduce_sum(ATTN, axis=1)\n",
    "        \n",
    "#         features2_ = tf.expand_dims(features2, 1)\n",
    "#         score = tf.nn.tanh(self.W1(features1) + self.W2(features2_))\n",
    "#         attention_weights = tf.nn.softmax(self.S(score), axis=1)\n",
    "#         ATTN = attention_weights * (features1 + features2)\n",
    "#         ATTN = tf.reduce_sum(ATTN, axis=1)\n",
    "        \n",
    "        return ATTN\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super(Attention_Cross, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0e5a047-2e7c-4200-978c-b3144c3a9cd9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:absl:`causal` argument is deprecated. Please use `use_causal_mask` in call() method to specify causal masking.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 36, 768)]    0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 10, 512)]    0           []                               \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 128, 512)]   0           []                               \n",
      "                                                                                                  \n",
      " model_14 (Functional)          [(None, 64),         295040      ['input_12[0][0]']               \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " model_12 (Functional)          (None, 64)           147904      ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " model_13 (Functional)          (None, 64)           123200      ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 128)          0           ['model_14[0][0]',               \n",
      "                                                                  'model_14[0][1]']               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 128)          0           ['model_12[0][0]',               \n",
      "                                                                  'model_13[0][0]']               \n",
      "                                                                                                  \n",
      " attention__cross (Attention_Cr  (None, 128)         33153       ['concatenate_3[0][0]',          \n",
      " oss)                                                             'concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 3)            387         ['attention__cross[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 599,684\n",
      "Trainable params: 33,540\n",
      "Non-trainable params: 566,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# './res/V_model_CNNLSTM_clip.tf'\n",
    "vis_ipt = Input((10, 512))\n",
    "v_model = tf.keras.models.load_model('./res/V_model_CNNLSTM_clip.tf/')\n",
    "v_model = Model(inputs=v_model.inputs, outputs=v_model.layers[-2].output)\n",
    "for l in v_model.layers:\n",
    "    l.trainable = False\n",
    "vis_h = v_model(vis_ipt)\n",
    "\n",
    "# './res/A_model_Att-BLSTM_wav2vec_v2.tf'\n",
    "aud_ipt = Input((128,512))\n",
    "a_model = tf.keras.models.load_model('./res/A_model_Att-BLSTM_wav2vec_v2.tf/')\n",
    "a_model = Model(inputs=a_model.inputs, outputs=a_model.layers[-2].output)\n",
    "for l in a_model.layers:\n",
    "    l.trainable = False\n",
    "aud_h = a_model(aud_ipt)\n",
    "\n",
    "# './res/T_model_AttCNN_bert.tf'\n",
    "tex_ipt = Input((36,768))\n",
    "t_model = tf.keras.models.load_model('./res/T_model_AttCNN_bert.tf/')\n",
    "t_model = Model(inputs=t_model.inputs, outputs=[t_model.layers[-4].output,t_model.layers[-3].output])\n",
    "for l in t_model.layers:\n",
    "    l.trainable = False\n",
    "tex_q, tex_qv_attention = t_model(tex_ipt)\n",
    "\n",
    "v = Concatenate()([tex_q, tex_qv_attention])\n",
    "q = Concatenate()([vis_h, aud_h])\n",
    "h = Attention_Cross(128)(v, q)\n",
    "# h = Dense(64)(h)\n",
    "res = Dense(3, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs=[vis_ipt, aud_ipt, tex_ipt], outputs=res)\n",
    "model.compile(optimizer='Adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics='acc')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4d75e4d-122f-4b6d-b7e9-9ddc08eaf1f8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.9647 - acc: 0.5473"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 11s 109ms/step - loss: 0.9645 - acc: 0.5482 - val_loss: 0.9768 - val_acc: 0.5680\n",
      "Epoch 2/30\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.7952 - acc: 0.6669"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 9s 101ms/step - loss: 0.7991 - acc: 0.6659 - val_loss: 0.9765 - val_acc: 0.5899\n",
      "Epoch 3/30\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.7473 - acc: 0.6791"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 8s 93ms/step - loss: 0.7491 - acc: 0.6784 - val_loss: 0.9620 - val_acc: 0.5833\n",
      "Epoch 4/30\n",
      "80/86 [==========================>...] - ETA: 0s - loss: 0.7056 - acc: 0.6961"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 16). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossatt_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 9s 102ms/step - loss: 0.7044 - acc: 0.6988 - val_loss: 0.9265 - val_acc: 0.6140\n",
      "Epoch 5/30\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.6756 - acc: 0.7200 - val_loss: 0.9943 - val_acc: 0.6272\n",
      "Epoch 6/30\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.6391 - acc: 0.7361 - val_loss: 0.9683 - val_acc: 0.6118\n",
      "Epoch 7/30\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.6083 - acc: 0.7420 - val_loss: 1.0022 - val_acc: 0.6140\n",
      "Epoch 8/30\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.6093 - acc: 0.7420 - val_loss: 1.0570 - val_acc: 0.6053\n",
      "Epoch 9/30\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 0.5938 - acc: 0.7427 - val_loss: 1.0551 - val_acc: 0.6053\n",
      "Epoch 10/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5968 - acc: 0.7537 - val_loss: 1.0327 - val_acc: 0.6031\n",
      "Epoch 11/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5804 - acc: 0.7654 - val_loss: 0.9892 - val_acc: 0.6250\n",
      "Epoch 12/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5810 - acc: 0.7485 - val_loss: 1.0527 - val_acc: 0.6118\n",
      "Epoch 13/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5663 - acc: 0.7573 - val_loss: 1.0538 - val_acc: 0.6118\n",
      "Epoch 14/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5806 - acc: 0.7588 - val_loss: 1.0085 - val_acc: 0.6228\n",
      "Epoch 15/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.5649 - acc: 0.7544 - val_loss: 1.0818 - val_acc: 0.5921\n",
      "Epoch 16/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5348 - acc: 0.7705 - val_loss: 1.0568 - val_acc: 0.6053\n",
      "Epoch 17/30\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5450 - acc: 0.7734 - val_loss: 1.0729 - val_acc: 0.6031\n",
      "Epoch 18/30\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5166 - acc: 0.7800 - val_loss: 1.0997 - val_acc: 0.6118\n",
      "Epoch 19/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5289 - acc: 0.7705 - val_loss: 1.0852 - val_acc: 0.6162\n",
      "Epoch 20/30\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.5144 - acc: 0.7829 - val_loss: 1.1125 - val_acc: 0.6009\n",
      "Epoch 21/30\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 0.5142 - acc: 0.7873 - val_loss: 1.1177 - val_acc: 0.5965\n",
      "Epoch 22/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4975 - acc: 0.7939 - val_loss: 1.1150 - val_acc: 0.6184\n",
      "Epoch 23/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5044 - acc: 0.7844 - val_loss: 1.0718 - val_acc: 0.6075\n",
      "Epoch 24/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4963 - acc: 0.7858 - val_loss: 1.1026 - val_acc: 0.6118\n",
      "Epoch 25/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5249 - acc: 0.7683 - val_loss: 1.1052 - val_acc: 0.5943\n",
      "Epoch 26/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4981 - acc: 0.7895 - val_loss: 1.1104 - val_acc: 0.5965\n",
      "Epoch 27/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.4958 - acc: 0.7990 - val_loss: 1.1171 - val_acc: 0.5987\n",
      "Epoch 28/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.4929 - acc: 0.8063 - val_loss: 1.1077 - val_acc: 0.5877\n",
      "Epoch 29/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.4704 - acc: 0.8063 - val_loss: 1.1937 - val_acc: 0.6009\n",
      "Epoch 30/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4684 - acc: 0.8121 - val_loss: 1.1621 - val_acc: 0.6009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3c07512d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_list = [ModelCheckpoint(filepath='./res/multi_model_crossatt_pretrained.tf', monitor='val_loss', save_best_only=True, save_freq='epoch')]\n",
    "\n",
    "model.fit(x=[np.asarray(visual_clip['train']), np.asarray(acoustic['train']), np.asarray(bert_embs['train'])], y=np.asarray(label['train']), batch_size=16, epochs=30, \n",
    "            validation_data=[[np.asarray(visual_clip['valid']), np.asarray(acoustic['valid']), np.asarray(bert_embs['valid'])], np.asarray(label['valid'])],\n",
    "            callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26f92f1a-ba96-4d45-94e4-2b6505e5a83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg     0.6717    0.8911    0.7660       248\n",
      "         Pos     0.6320    0.5643    0.5962       140\n",
      "         Neu     0.3333    0.0145    0.0278        69\n",
      "\n",
      "    accuracy                         0.6586       457\n",
      "   macro avg     0.5457    0.4900    0.4633       457\n",
      "weighted avg     0.6085    0.6586    0.6025       457\n",
      "\n",
      "[[221  27   0]\n",
      " [ 59  79   2]\n",
      " [ 49  19   1]]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./res/multi_model_crossatt_pretrained.tf/')\n",
    "pred = model.predict([np.asarray(visual_clip['test']), np.asarray(acoustic['test']), np.asarray(bert_embs['test'])])\n",
    "predicted_test_labels = pred.argmax(axis=1)\n",
    "numeric_test_labels = np.array(label['test'])\n",
    "            \n",
    "eval_res = classification_report(numeric_test_labels, predicted_test_labels, \n",
    "                                    target_names = ['Neg', 'Pos', 'Neu'], \n",
    "                                    digits=4, output_dict=False)\n",
    "\n",
    "print(eval_res)\n",
    "\n",
    "cm = confusion_matrix(y_true=numeric_test_labels.tolist(), y_pred=predicted_test_labels.tolist())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "583b12be-0776-45bd-b3dd-2a0c402c7ebb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:absl:`causal` argument is deprecated. Please use `use_causal_mask` in call() method to specify causal masking.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128) (None, 128)\n",
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 36, 768)]    0           []                               \n",
      "                                                                                                  \n",
      " input_13 (InputLayer)          [(None, 10, 512)]    0           []                               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 128, 512)]   0           []                               \n",
      "                                                                                                  \n",
      " model_18 (Functional)          [(None, 64),         295040      ['input_15[0][0]']               \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " model_16 (Functional)          (None, 64)           147904      ['input_13[0][0]']               \n",
      "                                                                                                  \n",
      " model_17 (Functional)          (None, 64)           123200      ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 128)          0           ['model_18[0][0]',               \n",
      "                                                                  'model_18[0][1]']               \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 128)          0           ['model_16[0][0]',               \n",
      "                                                                  'model_17[0][0]']               \n",
      "                                                                                                  \n",
      " attention_1 (Attention)        (None, 128)          1           ['concatenate_5[0][0]',          \n",
      "                                                                  'concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 3)            387         ['attention_1[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 566,532\n",
      "Trainable params: 388\n",
      "Non-trainable params: 566,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# './res/V_model_CNNLSTM_clip.tf'\n",
    "vis_ipt = Input((10, 512))\n",
    "v_model = tf.keras.models.load_model('./res/V_model_CNNLSTM_clip.tf/')\n",
    "v_model = Model(inputs=v_model.inputs, outputs=v_model.layers[-2].output)\n",
    "for l in v_model.layers:\n",
    "    l.trainable = False\n",
    "vis_h = v_model(vis_ipt)\n",
    "\n",
    "# './res/A_model_Att-BLSTM_wav2vec_v2.tf'\n",
    "aud_ipt = Input((128,512))\n",
    "a_model = tf.keras.models.load_model('./res/A_model_Att-BLSTM_wav2vec_v2.tf/')\n",
    "a_model = Model(inputs=a_model.inputs, outputs=a_model.layers[-2].output)\n",
    "for l in a_model.layers:\n",
    "    l.trainable = False\n",
    "aud_h = a_model(aud_ipt)\n",
    "\n",
    "# './res/T_model_AttCNN_bert.tf'\n",
    "tex_ipt = Input((36,768))\n",
    "t_model = tf.keras.models.load_model('./res/T_model_AttCNN_bert.tf/')\n",
    "t_model = Model(inputs=t_model.inputs, outputs=[t_model.layers[-4].output,t_model.layers[-3].output])\n",
    "for l in t_model.layers:\n",
    "    l.trainable = False\n",
    "tex_q, tex_qv_attention = t_model(tex_ipt)\n",
    "\n",
    "\n",
    "v = Concatenate()([tex_q, tex_qv_attention])\n",
    "q = Concatenate()([vis_h, aud_h])\n",
    "print(v.shape, q.shape)\n",
    "h = Attention(128)([v, q])\n",
    "# h = Dense(64)(h)\n",
    "res = Dense(3, activation='softmax')(h)\n",
    "\n",
    "model = Model(inputs=[vis_ipt, aud_ipt, tex_ipt], outputs=res)\n",
    "model.compile(optimizer='Adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics='acc')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7610657b-a43a-443a-8f3c-7da6e4451b84",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.1111 - acc: 0.4257"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossattv2_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossattv2_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 11s 105ms/step - loss: 1.1098 - acc: 0.4269 - val_loss: 0.9851 - val_acc: 0.5548\n",
      "Epoch 2/30\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.9572 - acc: 0.5453"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossattv2_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossattv2_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 8s 98ms/step - loss: 0.9572 - acc: 0.5453 - val_loss: 0.9802 - val_acc: 0.5570\n",
      "Epoch 3/30\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.9414 - acc: 0.5537"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossattv2_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossattv2_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 7s 85ms/step - loss: 0.9424 - acc: 0.5541 - val_loss: 0.9786 - val_acc: 0.5614\n",
      "Epoch 4/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.9469 - acc: 0.5556 - val_loss: 0.9811 - val_acc: 0.5548\n",
      "Epoch 5/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.9452 - acc: 0.5731 - val_loss: 0.9817 - val_acc: 0.5482\n",
      "Epoch 6/30\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.9474 - acc: 0.5640"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossattv2_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_crossattv2_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 9s 103ms/step - loss: 0.9465 - acc: 0.5651 - val_loss: 0.9777 - val_acc: 0.5482\n",
      "Epoch 7/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.9328 - acc: 0.5592 - val_loss: 0.9795 - val_acc: 0.5482\n",
      "Epoch 8/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.9213 - acc: 0.5892 - val_loss: 0.9815 - val_acc: 0.5482\n",
      "Epoch 9/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.9377 - acc: 0.5607 - val_loss: 0.9839 - val_acc: 0.5417\n",
      "Epoch 10/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.9265 - acc: 0.5658 - val_loss: 0.9865 - val_acc: 0.5482\n",
      "Epoch 11/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.9210 - acc: 0.5738 - val_loss: 0.9906 - val_acc: 0.5439\n",
      "Epoch 12/30\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.9313 - acc: 0.5746 - val_loss: 0.9920 - val_acc: 0.5439\n",
      "Epoch 13/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.9227 - acc: 0.5775 - val_loss: 0.9963 - val_acc: 0.5417\n",
      "Epoch 14/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.9089 - acc: 0.5950 - val_loss: 0.9937 - val_acc: 0.5417\n",
      "Epoch 15/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.9348 - acc: 0.5709 - val_loss: 0.9917 - val_acc: 0.5504\n",
      "Epoch 16/30\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.9108 - acc: 0.5892 - val_loss: 0.9986 - val_acc: 0.5461\n",
      "Epoch 17/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.9186 - acc: 0.5716 - val_loss: 0.9969 - val_acc: 0.5461\n",
      "Epoch 18/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.9293 - acc: 0.5658 - val_loss: 1.0002 - val_acc: 0.5461\n",
      "Epoch 19/30\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.9199 - acc: 0.5716 - val_loss: 1.0021 - val_acc: 0.5482\n",
      "Epoch 20/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.9247 - acc: 0.5841 - val_loss: 0.9942 - val_acc: 0.5504\n",
      "Epoch 21/30\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.9213 - acc: 0.5607 - val_loss: 0.9964 - val_acc: 0.5482\n",
      "Epoch 22/30\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.9110 - acc: 0.5789 - val_loss: 1.0011 - val_acc: 0.5548\n",
      "Epoch 23/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.9153 - acc: 0.5709 - val_loss: 1.0000 - val_acc: 0.5526\n",
      "Epoch 24/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.9112 - acc: 0.5870 - val_loss: 1.0009 - val_acc: 0.5548\n",
      "Epoch 25/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.9058 - acc: 0.5709 - val_loss: 1.0009 - val_acc: 0.5307\n",
      "Epoch 26/30\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.9095 - acc: 0.5804 - val_loss: 1.0028 - val_acc: 0.5526\n",
      "Epoch 27/30\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.9137 - acc: 0.5768 - val_loss: 1.0043 - val_acc: 0.5241\n",
      "Epoch 28/30\n",
      "86/86 [==============================] - 1s 15ms/step - loss: 0.9133 - acc: 0.5775 - val_loss: 1.0021 - val_acc: 0.5504\n",
      "Epoch 29/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.9095 - acc: 0.5870 - val_loss: 1.0066 - val_acc: 0.5351\n",
      "Epoch 30/30\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.8952 - acc: 0.5819 - val_loss: 1.0128 - val_acc: 0.5504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe371e515a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_list = [ModelCheckpoint(filepath='./res/multi_model_crossattv2_pretrained.tf', monitor='val_loss', save_best_only=True, save_freq='epoch')]\n",
    "\n",
    "model.fit(x=[np.asarray(visual_clip['train']), np.asarray(acoustic['train']), np.asarray(bert_embs['train'])], y=np.asarray(label['train']), batch_size=16, epochs=30, \n",
    "            validation_data=[[np.asarray(visual_clip['valid']), np.asarray(acoustic['valid']), np.asarray(bert_embs['valid'])], np.asarray(label['valid'])],\n",
    "            callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73326b0d-0498-4cd6-83a0-0bcb41fd1f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg     0.5754    0.9234    0.7090       248\n",
      "         Pos     0.5085    0.2143    0.3015       140\n",
      "         Neu     0.0000    0.0000    0.0000        69\n",
      "\n",
      "    accuracy                         0.5667       457\n",
      "   macro avg     0.3613    0.3792    0.3368       457\n",
      "weighted avg     0.4680    0.5667    0.4771       457\n",
      "\n",
      "[[229  19   0]\n",
      " [110  30   0]\n",
      " [ 59  10   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chason/anaconda3/envs/deepeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chason/anaconda3/envs/deepeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chason/anaconda3/envs/deepeye/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./res/multi_model_crossattv2_pretrained.tf/')\n",
    "pred = model.predict([np.asarray(visual_clip['test']), np.asarray(acoustic['test']), np.asarray(bert_embs['test'])])\n",
    "predicted_test_labels = pred.argmax(axis=1)\n",
    "numeric_test_labels = np.array(label['test'])\n",
    "            \n",
    "eval_res = classification_report(numeric_test_labels, predicted_test_labels, \n",
    "                                    target_names = ['Neg', 'Pos', 'Neu'], \n",
    "                                    digits=4, output_dict=False)\n",
    "\n",
    "print(eval_res)\n",
    "\n",
    "cm = confusion_matrix(y_true=numeric_test_labels.tolist(), y_pred=predicted_test_labels.tolist())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b72583e-58ea-418b-970d-3480e8344bc6",
   "metadata": {},
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d612c39-59ad-4f0a-b672-d773247f2028",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "        \n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context):\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "463a1f78-4383-4494-b623-90256c7112e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi_head Attention : Cross, Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "957351a4-6023-4702-bca7-9cb991c47632",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:absl:`causal` argument is deprecated. Please use `use_causal_mask` in call() method to specify causal masking.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_27\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 36, 768)]    0           []                               \n",
      "                                                                                                  \n",
      " input_19 (InputLayer)          [(None, 10, 512)]    0           []                               \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, 128, 512)]   0           []                               \n",
      "                                                                                                  \n",
      " model_26 (Functional)          [(None, 64),         295040      ['input_21[0][0]']               \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " model_24 (Functional)          (None, 64)           147904      ['input_19[0][0]']               \n",
      "                                                                                                  \n",
      " model_25 (Functional)          (None, 64)           123200      ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " tf.expand_dims_4 (TFOpLambda)  (None, 1, 64)        0           ['model_26[0][0]']               \n",
      "                                                                                                  \n",
      " tf.expand_dims_5 (TFOpLambda)  (None, 1, 64)        0           ['model_26[0][1]']               \n",
      "                                                                                                  \n",
      " tf.expand_dims_6 (TFOpLambda)  (None, 1, 64)        0           ['model_24[0][0]']               \n",
      "                                                                                                  \n",
      " tf.expand_dims_7 (TFOpLambda)  (None, 1, 64)        0           ['model_25[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 2, 64)        0           ['tf.expand_dims_4[0][0]',       \n",
      "                                                                  'tf.expand_dims_5[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 2, 64)        0           ['tf.expand_dims_6[0][0]',       \n",
      "                                                                  'tf.expand_dims_7[0][0]']       \n",
      "                                                                                                  \n",
      " cross_attention_1 (CrossAttent  (None, 2, 64)       33344       ['concatenate_9[0][0]',          \n",
      " ion)                                                             'concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_1 (TFOpLam  (None, 64)          0           ['cross_attention_1[0][0]']      \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 3)            195         ['tf.math.reduce_mean_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 599,683\n",
      "Trainable params: 33,539\n",
      "Non-trainable params: 566,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# './res/V_model_CNNLSTM_clip.tf'\n",
    "vis_ipt = Input((10, 512))\n",
    "v_model = tf.keras.models.load_model('./res/V_model_CNNLSTM_clip.tf/')\n",
    "v_model = Model(inputs=v_model.inputs, outputs=v_model.layers[-2].output)\n",
    "for l in v_model.layers:\n",
    "    l.trainable = False\n",
    "vis_h = v_model(vis_ipt)\n",
    "\n",
    "# './res/A_model_Att-BLSTM_wav2vec_v2.tf'\n",
    "aud_ipt = Input((128,512))\n",
    "a_model = tf.keras.models.load_model('./res/A_model_Att-BLSTM_wav2vec_v2.tf/')\n",
    "a_model = Model(inputs=a_model.inputs, outputs=a_model.layers[-2].output)\n",
    "for l in a_model.layers:\n",
    "    l.trainable = False\n",
    "aud_h = a_model(aud_ipt)\n",
    "\n",
    "# './res/T_model_AttCNN_bert.tf'\n",
    "tex_ipt = Input((36,768))\n",
    "t_model = tf.keras.models.load_model('./res/T_model_AttCNN_bert.tf/')\n",
    "t_model = Model(inputs=t_model.inputs, outputs=[t_model.layers[-4].output,t_model.layers[-3].output])\n",
    "for l in t_model.layers:\n",
    "    l.trainable = False\n",
    "tex_q, tex_qv_attention = t_model(tex_ipt)\n",
    "\n",
    "v = Concatenate(axis=1)([tf.expand_dims(tex_q,1), tf.expand_dims(tex_qv_attention,1)])\n",
    "q = Concatenate(axis=1)([tf.expand_dims(vis_h,1), tf.expand_dims(aud_h,1)])\n",
    "\n",
    "# h = MultiHeadAttention(num_heads=4, key_dim=32, dropout=0.2)(v, q)\n",
    "h = CrossAttention(num_heads=4, key_dim=32, dropout=0.2)(v, q)\n",
    "\n",
    "# h = Dense(64)(h)\n",
    "# res = Dense(3, activation='softmax')(h)\n",
    "res = Dense(3, activation='softmax')(tf.reduce_mean(h, 1))\n",
    "\n",
    "model = Model(inputs=[vis_ipt, aud_ipt, tex_ipt], outputs=res)\n",
    "model.compile(optimizer='Adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics='acc')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e001b45-d5a6-4e84-9342-5235d8da8c10",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.7403 - acc: 0.6805"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_multiheadatt_cross_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_multiheadatt_cross_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 11s 80ms/step - loss: 0.7403 - acc: 0.6805 - val_loss: 0.9297 - val_acc: 0.6053\n",
      "Epoch 2/30\n",
      "115/115 [==============================] - ETA: 0s - loss: 0.5828 - acc: 0.7627"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_multiheadatt_cross_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_multiheadatt_cross_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 9s 81ms/step - loss: 0.5828 - acc: 0.7627 - val_loss: 0.9068 - val_acc: 0.6425\n",
      "Epoch 3/30\n",
      "115/115 [==============================] - 1s 13ms/step - loss: 0.5443 - acc: 0.7770 - val_loss: 0.9552 - val_acc: 0.6491\n",
      "Epoch 4/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.5272 - acc: 0.7775 - val_loss: 0.9549 - val_acc: 0.6491\n",
      "Epoch 5/30\n",
      "115/115 [==============================] - 1s 9ms/step - loss: 0.5124 - acc: 0.7989 - val_loss: 0.9349 - val_acc: 0.6645\n",
      "Epoch 6/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.4986 - acc: 0.8011 - val_loss: 0.9588 - val_acc: 0.6491\n",
      "Epoch 7/30\n",
      "115/115 [==============================] - 1s 9ms/step - loss: 0.4836 - acc: 0.8044 - val_loss: 0.9250 - val_acc: 0.6425\n",
      "Epoch 8/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.4839 - acc: 0.8044 - val_loss: 0.9510 - val_acc: 0.6579\n",
      "Epoch 9/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.4595 - acc: 0.8236 - val_loss: 0.9861 - val_acc: 0.6645\n",
      "Epoch 10/30\n",
      "115/115 [==============================] - 1s 9ms/step - loss: 0.4637 - acc: 0.8132 - val_loss: 0.9754 - val_acc: 0.6316\n",
      "Epoch 11/30\n",
      "115/115 [==============================] - 1s 9ms/step - loss: 0.4555 - acc: 0.8060 - val_loss: 0.9291 - val_acc: 0.6491\n",
      "Epoch 12/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.4333 - acc: 0.8301 - val_loss: 1.0401 - val_acc: 0.6162\n",
      "Epoch 13/30\n",
      "115/115 [==============================] - 1s 13ms/step - loss: 0.4374 - acc: 0.8274 - val_loss: 1.0182 - val_acc: 0.6469\n",
      "Epoch 14/30\n",
      "115/115 [==============================] - 2s 14ms/step - loss: 0.4286 - acc: 0.8307 - val_loss: 0.9882 - val_acc: 0.6579\n",
      "Epoch 15/30\n",
      "115/115 [==============================] - 2s 15ms/step - loss: 0.4223 - acc: 0.8312 - val_loss: 1.0007 - val_acc: 0.6338\n",
      "Epoch 16/30\n",
      "115/115 [==============================] - 2s 14ms/step - loss: 0.4001 - acc: 0.8493 - val_loss: 1.0731 - val_acc: 0.6469\n",
      "Epoch 17/30\n",
      "115/115 [==============================] - 2s 15ms/step - loss: 0.4139 - acc: 0.8285 - val_loss: 1.0804 - val_acc: 0.6272\n",
      "Epoch 18/30\n",
      "115/115 [==============================] - 2s 16ms/step - loss: 0.4115 - acc: 0.8362 - val_loss: 1.0498 - val_acc: 0.6360\n",
      "Epoch 19/30\n",
      "115/115 [==============================] - 2s 15ms/step - loss: 0.3886 - acc: 0.8488 - val_loss: 1.0349 - val_acc: 0.6579\n",
      "Epoch 20/30\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.3827 - acc: 0.8504 - val_loss: 1.0760 - val_acc: 0.6206\n",
      "Epoch 21/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.3924 - acc: 0.8471 - val_loss: 1.0766 - val_acc: 0.6579\n",
      "Epoch 22/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.3764 - acc: 0.8532 - val_loss: 1.0582 - val_acc: 0.6316\n",
      "Epoch 23/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.3581 - acc: 0.8652 - val_loss: 1.1157 - val_acc: 0.6425\n",
      "Epoch 24/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.3594 - acc: 0.8663 - val_loss: 1.1155 - val_acc: 0.6579\n",
      "Epoch 25/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.3566 - acc: 0.8564 - val_loss: 1.1585 - val_acc: 0.6360\n",
      "Epoch 26/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.3629 - acc: 0.8537 - val_loss: 1.1200 - val_acc: 0.6447\n",
      "Epoch 27/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.3475 - acc: 0.8570 - val_loss: 1.1175 - val_acc: 0.6579\n",
      "Epoch 28/30\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.3413 - acc: 0.8674 - val_loss: 1.2805 - val_acc: 0.6206\n",
      "Epoch 29/30\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.3325 - acc: 0.8816 - val_loss: 1.1999 - val_acc: 0.6228\n",
      "Epoch 30/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.3386 - acc: 0.8690 - val_loss: 1.1529 - val_acc: 0.6404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe38a573340>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_list = [ModelCheckpoint(filepath='./res/multi_model_multiheadatt_cross_pretrained.tf', monitor='val_loss', save_best_only=True, save_freq='epoch')]\n",
    "\n",
    "model.fit(x=[np.asarray(visual_clip['train']), np.asarray(acoustic['train']), np.asarray(bert_embs['train'])], \n",
    "          y=np.asarray(label['train']), batch_size=16, epochs=30,\n",
    "          validation_data=[[np.asarray(visual_clip['valid']), np.asarray(acoustic['valid']), np.asarray(bert_embs['valid'])],np.asarray(label['valid'])],\n",
    "          callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1e7ec3c-f6f8-460c-8f40-ea1a00af2d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg     0.7276    0.8508    0.7844       248\n",
      "         Pos     0.6838    0.5714    0.6226       140\n",
      "         Neu     0.4000    0.2899    0.3361        69\n",
      "\n",
      "    accuracy                         0.6805       457\n",
      "   macro avg     0.6038    0.5707    0.5810       457\n",
      "weighted avg     0.6647    0.6805    0.6671       457\n",
      "\n",
      "[[211  22  15]\n",
      " [ 45  80  15]\n",
      " [ 34  15  20]]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./res/multi_model_multiheadatt_cross_pretrained.tf/')\n",
    "pred = model.predict([np.asarray(visual_clip['test']), np.asarray(acoustic['test']), np.asarray(bert_embs['test'])])\n",
    "predicted_test_labels = pred.argmax(axis=1)\n",
    "numeric_test_labels = np.array(label['test'])\n",
    "            \n",
    "eval_res = classification_report(numeric_test_labels, predicted_test_labels, \n",
    "                                    target_names = ['Neg', 'Pos', 'Neu'], \n",
    "                                    digits=4, output_dict=False)\n",
    "\n",
    "print(eval_res)\n",
    "\n",
    "cm = confusion_matrix(y_true=numeric_test_labels.tolist(), y_pred=predicted_test_labels.tolist())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a8debd1-e8b5-465c-b914-60dce5c53719",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:absl:`causal` argument is deprecated. Please use `use_causal_mask` in call() method to specify causal masking.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_35\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 36, 768)]    0           []                               \n",
      "                                                                                                  \n",
      " input_25 (InputLayer)          [(None, 10, 512)]    0           []                               \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)          [(None, 128, 512)]   0           []                               \n",
      "                                                                                                  \n",
      " model_34 (Functional)          [(None, 64),         295040      ['input_27[0][0]']               \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " model_32 (Functional)          (None, 64)           147904      ['input_25[0][0]']               \n",
      "                                                                                                  \n",
      " model_33 (Functional)          (None, 64)           123200      ['input_26[0][0]']               \n",
      "                                                                                                  \n",
      " tf.expand_dims_12 (TFOpLambda)  (None, 1, 64)       0           ['model_34[0][0]']               \n",
      "                                                                                                  \n",
      " tf.expand_dims_13 (TFOpLambda)  (None, 1, 64)       0           ['model_34[0][1]']               \n",
      "                                                                                                  \n",
      " tf.expand_dims_14 (TFOpLambda)  (None, 1, 64)       0           ['model_32[0][0]']               \n",
      "                                                                                                  \n",
      " tf.expand_dims_15 (TFOpLambda)  (None, 1, 64)       0           ['model_33[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 4, 64)        0           ['tf.expand_dims_12[0][0]',      \n",
      "                                                                  'tf.expand_dims_13[0][0]',      \n",
      "                                                                  'tf.expand_dims_14[0][0]',      \n",
      "                                                                  'tf.expand_dims_15[0][0]']      \n",
      "                                                                                                  \n",
      " global_self_attention_1 (Globa  (None, 4, 64)       33344       ['concatenate_12[0][0]']         \n",
      " lSelfAttention)                                                                                  \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_3 (TFOpLam  (None, 64)          0           ['global_self_attention_1[0][0]']\n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 3)            195         ['tf.math.reduce_mean_3[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 599,683\n",
      "Trainable params: 33,539\n",
      "Non-trainable params: 566,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# './res/V_model_CNNLSTM_clip.tf'\n",
    "vis_ipt = Input((10, 512))\n",
    "v_model = tf.keras.models.load_model('./res/V_model_CNNLSTM_clip.tf/')\n",
    "v_model = Model(inputs=v_model.inputs, outputs=v_model.layers[-2].output)\n",
    "for l in v_model.layers:\n",
    "    l.trainable = False\n",
    "vis_h = v_model(vis_ipt)\n",
    "\n",
    "# './res/A_model_Att-BLSTM_wav2vec_v2.tf'\n",
    "aud_ipt = Input((128,512))\n",
    "a_model = tf.keras.models.load_model('./res/A_model_Att-BLSTM_wav2vec_v2.tf/')\n",
    "a_model = Model(inputs=a_model.inputs, outputs=a_model.layers[-2].output)\n",
    "for l in a_model.layers:\n",
    "    l.trainable = False\n",
    "aud_h = a_model(aud_ipt)\n",
    "\n",
    "# './res/T_model_AttCNN_bert.tf'\n",
    "tex_ipt = Input((36,768))\n",
    "t_model = tf.keras.models.load_model('./res/T_model_AttCNN_bert.tf/')\n",
    "t_model = Model(inputs=t_model.inputs, outputs=[t_model.layers[-4].output,t_model.layers[-3].output])\n",
    "for l in t_model.layers:\n",
    "    l.trainable = False\n",
    "tex_q, tex_qv_attention = t_model(tex_ipt)\n",
    "\n",
    "h = Concatenate(axis=1)([tf.expand_dims(tex_q,1), tf.expand_dims(tex_qv_attention,1), tf.expand_dims(vis_h,1), tf.expand_dims(aud_h,1)])\n",
    "\n",
    "h = GlobalSelfAttention(num_heads=4, key_dim=32, dropout=0.2)(h)\n",
    "\n",
    "# h = Dense(64)(h)\n",
    "# res = Dense(3, activation='softmax')(h)\n",
    "res = Dense(3, activation='softmax')(tf.reduce_mean(h, 1))\n",
    "\n",
    "model = Model(inputs=[vis_ipt, aud_ipt, tex_ipt], outputs=res)\n",
    "model.compile(optimizer='Adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics='acc')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90aa3dd4-8330-48d5-923d-b23c98ddd977",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "111/115 [===========================>..] - ETA: 0s - loss: 0.6876 - acc: 0.7072"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 28). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_multiheadatt_global_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./res/multi_model_multiheadatt_global_pretrained.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 12s 77ms/step - loss: 0.6833 - acc: 0.7112 - val_loss: 0.9500 - val_acc: 0.6228\n",
      "Epoch 2/30\n",
      "115/115 [==============================] - 1s 13ms/step - loss: 0.5547 - acc: 0.7797 - val_loss: 0.9874 - val_acc: 0.6382\n",
      "Epoch 3/30\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.5273 - acc: 0.7945 - val_loss: 0.9769 - val_acc: 0.6447\n",
      "Epoch 4/30\n",
      "115/115 [==============================] - 1s 11ms/step - loss: 0.5125 - acc: 0.8066 - val_loss: 0.9728 - val_acc: 0.6294\n",
      "Epoch 5/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.4865 - acc: 0.8099 - val_loss: 1.0281 - val_acc: 0.6250\n",
      "Epoch 6/30\n",
      "115/115 [==============================] - 1s 11ms/step - loss: 0.4814 - acc: 0.8115 - val_loss: 0.9888 - val_acc: 0.6469\n",
      "Epoch 7/30\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.4634 - acc: 0.8153 - val_loss: 1.0436 - val_acc: 0.6404\n",
      "Epoch 8/30\n",
      "115/115 [==============================] - 2s 15ms/step - loss: 0.4676 - acc: 0.8175 - val_loss: 1.0863 - val_acc: 0.6425\n",
      "Epoch 9/30\n",
      "115/115 [==============================] - 1s 13ms/step - loss: 0.4438 - acc: 0.8318 - val_loss: 1.0172 - val_acc: 0.6557\n",
      "Epoch 10/30\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.4359 - acc: 0.8247 - val_loss: 1.0195 - val_acc: 0.6469\n",
      "Epoch 11/30\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.4299 - acc: 0.8274 - val_loss: 1.1144 - val_acc: 0.6447\n",
      "Epoch 12/30\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.4182 - acc: 0.8340 - val_loss: 1.0156 - val_acc: 0.6425\n",
      "Epoch 13/30\n",
      "115/115 [==============================] - 1s 13ms/step - loss: 0.4045 - acc: 0.8405 - val_loss: 1.0804 - val_acc: 0.6535\n",
      "Epoch 14/30\n",
      "115/115 [==============================] - 1s 12ms/step - loss: 0.3926 - acc: 0.8455 - val_loss: 1.0765 - val_acc: 0.6557\n",
      "Epoch 15/30\n",
      "115/115 [==============================] - 1s 11ms/step - loss: 0.3954 - acc: 0.8422 - val_loss: 1.1094 - val_acc: 0.6469\n",
      "Epoch 16/30\n",
      "115/115 [==============================] - 1s 11ms/step - loss: 0.3803 - acc: 0.8510 - val_loss: 1.0893 - val_acc: 0.6425\n",
      "Epoch 17/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.3664 - acc: 0.8548 - val_loss: 1.1225 - val_acc: 0.6535\n",
      "Epoch 18/30\n",
      "115/115 [==============================] - 1s 11ms/step - loss: 0.3687 - acc: 0.8510 - val_loss: 1.1991 - val_acc: 0.6228\n",
      "Epoch 19/30\n",
      "115/115 [==============================] - 2s 15ms/step - loss: 0.3623 - acc: 0.8586 - val_loss: 1.1055 - val_acc: 0.6557\n",
      "Epoch 20/30\n",
      "115/115 [==============================] - 2s 15ms/step - loss: 0.3641 - acc: 0.8515 - val_loss: 1.1549 - val_acc: 0.6206\n",
      "Epoch 21/30\n",
      "115/115 [==============================] - 2s 16ms/step - loss: 0.3371 - acc: 0.8668 - val_loss: 1.1912 - val_acc: 0.6623\n",
      "Epoch 22/30\n",
      "115/115 [==============================] - 2s 16ms/step - loss: 0.3253 - acc: 0.8767 - val_loss: 1.1615 - val_acc: 0.6404\n",
      "Epoch 23/30\n",
      "115/115 [==============================] - 2s 15ms/step - loss: 0.3234 - acc: 0.8712 - val_loss: 1.2213 - val_acc: 0.6228\n",
      "Epoch 24/30\n",
      "115/115 [==============================] - 2s 16ms/step - loss: 0.3122 - acc: 0.8866 - val_loss: 1.2810 - val_acc: 0.6579\n",
      "Epoch 25/30\n",
      "115/115 [==============================] - 2s 14ms/step - loss: 0.3064 - acc: 0.8745 - val_loss: 1.2620 - val_acc: 0.6404\n",
      "Epoch 26/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.3075 - acc: 0.8838 - val_loss: 1.2756 - val_acc: 0.6382\n",
      "Epoch 27/30\n",
      "115/115 [==============================] - 1s 11ms/step - loss: 0.2948 - acc: 0.8871 - val_loss: 1.3713 - val_acc: 0.6447\n",
      "Epoch 28/30\n",
      "115/115 [==============================] - 1s 11ms/step - loss: 0.3017 - acc: 0.8805 - val_loss: 1.3724 - val_acc: 0.6404\n",
      "Epoch 29/30\n",
      "115/115 [==============================] - 1s 10ms/step - loss: 0.3084 - acc: 0.8795 - val_loss: 1.2946 - val_acc: 0.6513\n",
      "Epoch 30/30\n",
      "115/115 [==============================] - 1s 11ms/step - loss: 0.2640 - acc: 0.8997 - val_loss: 1.4275 - val_acc: 0.6447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe39b1aeef0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_list = [ModelCheckpoint(filepath='./res/multi_model_multiheadatt_global_pretrained.tf', monitor='val_loss', save_best_only=True, save_freq='epoch')]\n",
    "\n",
    "model.fit(x=[np.asarray(visual_clip['train']), np.asarray(acoustic['train']), np.asarray(bert_embs['train'])], \n",
    "          y=np.asarray(label['train']), batch_size=16, epochs=30,\n",
    "          validation_data=[[np.asarray(visual_clip['valid']), np.asarray(acoustic['valid']), np.asarray(bert_embs['valid'])],np.asarray(label['valid'])],\n",
    "          callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87fd701b-689b-4ff1-a646-0772be61f4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg     0.7529    0.7742    0.7634       248\n",
      "         Pos     0.5965    0.7286    0.6559       140\n",
      "         Neu     0.3871    0.1739    0.2400        69\n",
      "\n",
      "    accuracy                         0.6696       457\n",
      "   macro avg     0.5788    0.5589    0.5531       457\n",
      "weighted avg     0.6498    0.6696    0.6515       457\n",
      "\n",
      "[[192  48   8]\n",
      " [ 27 102  11]\n",
      " [ 36  21  12]]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./res/multi_model_multiheadatt_global_pretrained.tf/')\n",
    "pred = model.predict([np.asarray(visual_clip['test']), np.asarray(acoustic['test']), np.asarray(bert_embs['test'])])\n",
    "predicted_test_labels = pred.argmax(axis=1)\n",
    "numeric_test_labels = np.array(label['test'])\n",
    "            \n",
    "eval_res = classification_report(numeric_test_labels, predicted_test_labels, \n",
    "                                    target_names = ['Neg', 'Pos', 'Neu'], \n",
    "                                    digits=4, output_dict=False)\n",
    "\n",
    "print(eval_res)\n",
    "\n",
    "cm = confusion_matrix(y_true=numeric_test_labels.tolist(), y_pred=predicted_test_labels.tolist())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50726d-802d-4252-8abf-b28858dff020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
