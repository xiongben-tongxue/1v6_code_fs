{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a513d7-11d1-442b-aa53-0e9e18bad365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "import jieba\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from utilz import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cfbac95-d047-412c-8d98-34ec6b5dfc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids, clip_ids, texts, annotations, modes = load_data('../SIMS/label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2cdd39e-8a37-4416-b64a-160a55e3ba04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我不想嫁给李茶',\n",
       " '你这是嫁入豪门啊！',\n",
       " '我不想嫁入什么豪门，我们不就是豪门吗？',\n",
       " '有那么明显吗？',\n",
       " '我在这消费这么多钱，我低血糖吃块糖还不行了']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef25bc90-1b95-40fa-8588-21deb53faacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.380 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '不想', '嫁给', '李茶']\n"
     ]
    }
   ],
   "source": [
    "example_seg = list(jieba.lcut(texts[0]))\n",
    "print(example_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e8796-0022-4b31-a4ca-42b76d8aa734",
   "metadata": {},
   "source": [
    "# Construct a vocabulary and train a word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4713fbe-f946-4763-a9f0-2a184a03337c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2281\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "vocabs = []\n",
    "max_len = 0\n",
    "for s in texts:\n",
    "    tokens = list(jieba.lcut(s))\n",
    "    vocabs.append(tokens)\n",
    "    if len(tokens) > max_len:\n",
    "        max_len = len(tokens)\n",
    "    \n",
    "print(len(vocabs))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "206b66ef-7932-4311-83cd-94602e29a412",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['我', '不想', '嫁给', '李茶'],\n",
       " ['你', '这', '是', '嫁入', '豪门', '啊', '！'],\n",
       " ['我', '不想', '嫁入', '什么', '豪门', '，', '我们', '不', '就是', '豪门', '吗', '？'],\n",
       " ['有', '那么', '明显', '吗', '？'],\n",
       " ['我', '在', '这', '消费', '这么', '多钱', '，', '我', '低血糖', '吃块', '糖', '还', '不行', '了'],\n",
       " ['够',\n",
       "  '了',\n",
       "  '，',\n",
       "  '每次',\n",
       "  '都',\n",
       "  '是',\n",
       "  '这',\n",
       "  '一套',\n",
       "  '，',\n",
       "  '我',\n",
       "  '不想',\n",
       "  '再',\n",
       "  '听',\n",
       "  '了',\n",
       "  '。'],\n",
       " ['我',\n",
       "  '刚才',\n",
       "  '信号',\n",
       "  '不好',\n",
       "  '，',\n",
       "  '现在',\n",
       "  '可以',\n",
       "  '了',\n",
       "  '，',\n",
       "  '怎么样',\n",
       "  '，',\n",
       "  '妈',\n",
       "  '，',\n",
       "  '公司',\n",
       "  '给',\n",
       "  '我',\n",
       "  '安排',\n",
       "  '的',\n",
       "  '海景',\n",
       "  '别墅',\n",
       "  '楼',\n",
       "  '。'],\n",
       " ['我', '刚才', '是', '一', '害怕', '我', '就', '跟', '王安迪', '撒', '了', '个', '谎'],\n",
       " ['说',\n",
       "  '到',\n",
       "  '演讲',\n",
       "  '啊',\n",
       "  '，',\n",
       "  '我',\n",
       "  '不禁',\n",
       "  '想起',\n",
       "  '了',\n",
       "  '我',\n",
       "  '在',\n",
       "  '我',\n",
       "  '的',\n",
       "  '母校',\n",
       "  '。'],\n",
       " ['经理', '经理', '，', '别忘了', '我', '那', '五十万', '和', '副科长', '。']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "938cc853-ffd5-405e-a781-118e3e1c8d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168624, 241620)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_own = Word2Vec(sentences=vocabs, vector_size=100, sg=1, min_count=1)\n",
    "model_own.save(\"word2vec.model\")\n",
    "model_own.train(vocabs, total_examples=24162, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "092af266-3639-4b95-a0ef-5570a5eed24f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_embs = {'train':[], 'valid':[], 'test':[]}\n",
    "\n",
    "for s, mode in zip(vocabs, modes):\n",
    "    tmp_embs = []\n",
    "    for w in s:\n",
    "        tmp_embs.append(model_own.wv[w])\n",
    "    if len(tmp_embs)<max_len:\n",
    "        tmp_embs = np.concatenate([tmp_embs, np.zeros((max_len-len(tmp_embs), 100))], axis=0)\n",
    "    word_embs[mode].append(tmp_embs[:max_len])\n",
    "    \n",
    "save_features(word_embs, './data/textual_wav2vec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c286b4-d88b-407a-9403-5007cfa59f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1368, 36, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(word_embs['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eefef8c-6201-441e-8b48-8db0a1ffee2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.downloader.info()['models'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a60e9f3c-7ebb-45b3-b93e-b125ca070c1b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===-----------------------------------------------] 6.2% 23.4/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========------------------------------------------] 17.3% 65.0/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============-------------------------------------] 26.3% 99.1/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============-----------------------------------] 31.1% 117.0/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================-------------------------------] 39.2% 147.4/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================---------------------------] 47.7% 179.4/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================----------------------] 56.2% 211.4/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove_300 = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2f01ded-32e7-4042-a859-e926e36d0b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2220246"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_300.similarity('friend', 'enemy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af26e9b5-c1aa-4262-b6ac-dd426aa04190",
   "metadata": {},
   "source": [
    "# Word Embedding with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8f45f91-df52-4e08-8d1d-52824c5f2c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091cc860073441b38ba3d50b82595165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15af14978e1840ab90490b43bfc73047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4454462210d4330a2578898545dc21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac3d51cd1d14bdba6f6953acb156c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f53353b6f74732b281c2496d51e5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\", output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc840b2-37d4-46d2-b3ae-4ebcedac617f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_embs = {'train':[], 'valid':[], 'test':[]}\n",
    "\n",
    "for text, mode in zip(texts, modes):\n",
    "    # print(text)\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\" \n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        tmp_embs = outputs[-1][0].cpu().detach().numpy()[0] #（N，768）\n",
    "    if len(tmp_embs) < max_len:\n",
    "        tmp_embs = np.concatenate([tmp_embs, np.zeros((max_len-len(tmp_embs), 768))], axis=0)\n",
    "    word_embs[mode].append(tmp_embs[:max_len])\n",
    "    \n",
    "save_features(word_embs, './data/textual_bert.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc58e810-50d1-4d01-aa9f-8caf0d42d32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1368, 36, 768)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(word_embs['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9bc345-c9e6-4ff1-b010-6ec3c0a8011b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
